{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3a2ad5-fac8-4e5e-b7d4-70972816357e",
   "metadata": {},
   "source": [
    "# one_hierarchical_list_generation.py\n",
    "\n",
    "## Parsowanie listy hierarchicznej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7511664f-1474-4d47-b30d-9fc99e007c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headings:\n",
      "\n",
      "* Introduction\n",
      "* Efficient Data Management\n",
      "* Conclusion\n",
      "\n",
      "Subheadings:\n",
      "\n",
      "* Explanation of data engineering\n",
      "* Importance of data engineering in today’s data-driven world\n",
      "* Definition of data management\n",
      "* How data engineering helps in efficient data management.\n",
      "* Importance of Data Engineering in the modern business world\n",
      "* Future of Data Engineering and its impact on the data ecosystem\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOpis działania kodu:\\n\\nKod ten analizuje tekst zawierający konspekt artykułu i wyodrębnia z niego nagłówki oraz podnagłówki za pomocą wyrażeń regularnych.\\nNajpierw definiujemy wzorce do rozpoznawania nagłówków (sekcji oznaczonych * np. “Introduction”) i podnagłówków (elementów oznaczonych a., b. itp.).\\nNastępnie, za pomocą funkcji re.findall(), wyszukujemy pasujące fragmenty w tekście i wyświetlamy je w czytelnej formie.\\n\\nZastosowanie kodu:\\n\\t•\\tMoże być używany do parsowania tekstu z generowanych treści AI.\\n\\t•\\tPrzydaje się w przetwarzaniu tekstu, np. do ekstrakcji informacji ze strukturalnych dokumentów.\\n\\t•\\tMoże być wykorzystany do automatycznej analizy dokumentacji, np. w systemach zarządzania treścią.\\n    '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  # Importujemy moduł `re`, który służy do pracy z wyrażeniami regularnymi w Pythonie.\n",
    "\n",
    "# Przykładowy wynik generacji konspektu artykułu przez OpenAI\n",
    "openai_result = \"\"\"\n",
    "* Introduction\n",
    "    a. Explanation of data engineering\n",
    "    b. Importance of data engineering in today’s data-driven world\n",
    "* Efficient Data Management\n",
    "    a. Definition of data management\n",
    "    b. How data engineering helps in efficient data management.\n",
    "* Conclusion\n",
    "    a. Importance of Data Engineering in the modern business world\n",
    "    b. Future of Data Engineering and its impact on the data ecosystem\n",
    "\"\"\"\n",
    "\n",
    "# Definiujemy wzorce wyrażeń regularnych do wyszukiwania nagłówków i podnagłówków\n",
    "heading_pattern = r\"\\* (.+)\"  # Wzorzec dopasowuje linie zaczynające się od '* ' i przechwytuje treść po gwiazdce.\n",
    "subheading_pattern = r\"\\s+[a-z]\\. (.+)\"  # Wzorzec dopasowuje podpunkty zaczynające się od litery, kropki i spacji.\n",
    "\n",
    "# Wyszukujemy wszystkie nagłówki pasujące do wzorca `heading_pattern`\n",
    "headings = re.findall(heading_pattern, openai_result)\n",
    "\n",
    "# Wyszukujemy wszystkie podnagłówki pasujące do wzorca `subheading_pattern`\n",
    "subheadings = re.findall(subheading_pattern, openai_result)\n",
    "\n",
    "# Wyświetlamy znalezione nagłówki\n",
    "print(\"Headings:\\n\")  # Drukujemy nagłówek sekcji\n",
    "for heading in headings:\n",
    "    print(f\"* {heading}\")  # Drukujemy każdy nagłówek, dodając przed nim '* '\n",
    "\n",
    "# Wyświetlamy znalezione podnagłówki\n",
    "print(\"\\nSubheadings:\\n\")  # Przerwa między sekcjami i drukujemy nagłówek sekcji podnagłówków\n",
    "for subheading in subheadings:\n",
    "    print(f\"* {subheading}\")  # Drukujemy każdy podnagłówek, dodając przed nim '* '\n",
    "\n",
    "\n",
    "'''\n",
    "Opis działania kodu:\n",
    "\n",
    "Kod ten analizuje tekst zawierający konspekt artykułu i wyodrębnia z niego nagłówki oraz podnagłówki za pomocą wyrażeń regularnych.\n",
    "Najpierw definiujemy wzorce do rozpoznawania nagłówków (sekcji oznaczonych * np. “Introduction”) i podnagłówków (elementów oznaczonych a., b. itp.).\n",
    "Następnie, za pomocą funkcji re.findall(), wyszukujemy pasujące fragmenty w tekście i wyświetlamy je w czytelnej formie.\n",
    "\n",
    "Zastosowanie kodu:\n",
    "\t•\tMoże być używany do parsowania tekstu z generowanych treści AI.\n",
    "\t•\tPrzydaje się w przetwarzaniu tekstu, np. do ekstrakcji informacji ze strukturalnych dokumentów.\n",
    "\t•\tMoże być wykorzystany do automatycznej analizy dokumentacji, np. w systemach zarządzania treścią.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87f814-d3b8-45da-8b16-96f96170473d",
   "metadata": {},
   "source": [
    "# two_hierarchical_list_generation.py\n",
    "\n",
    "## Parsowanie hierarchicznej listy do słownika w Pythonie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a8ed55-eaa5-40ea-8822-71f27a02cf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Introduction': ['a. Explanation of data engineering', 'b. Importance of data engineering in today’s data-driven world'], 'Efficient Data Management': ['a. Definition of data management', 'b. How data engineering helps in efficient data management.', 'c. Why data engineering is important for data management'], 'Conclusion': ['a. Importance of Data Engineering in the modern business world', 'b. Future of Data Engineering and its impact on the data ecosystem']}\n"
     ]
    }
   ],
   "source": [
    "import re  # Importujemy moduł `re`, który służy do obsługi wyrażeń regularnych w Pythonie.\n",
    "\n",
    "# Przykładowy tekst zawierający konspekt artykułu wygenerowany przez OpenAI\n",
    "openai_result = \"\"\"\n",
    "* Introduction\n",
    "  a. Explanation of data engineering\n",
    "  b. Importance of data engineering in today’s data-driven world\n",
    "* Efficient Data Management\n",
    "    a. Definition of data management\n",
    "    b. How data engineering helps in efficient data management.\n",
    "    c. Why data engineering is important for data management\n",
    "* Conclusion\n",
    "    a. Importance of Data Engineering in the modern business world\n",
    "    b. Future of Data Engineering and its impact on the data ecosystem\n",
    "\"\"\"\n",
    "\n",
    "# Tworzymy wyrażenie regularne do dopasowania sekcji (główne nagłówki oznaczone \"* \")\n",
    "section_regex = re.compile(r\"\\* (.+)\")\n",
    "\n",
    "# Tworzymy wyrażenie regularne do dopasowania podsekcji (podpunkty oznaczone literą i kropką, np. \"a. \")\n",
    "subsection_regex = re.compile(r\"\\s*([a-z]\\..+)\")\n",
    "\n",
    "# Inicjalizujemy pusty słownik, w którym przechowywane będą sekcje i ich podsekcje\n",
    "result_dict = {}\n",
    "\n",
    "# Zmienna pomocnicza do śledzenia bieżącej sekcji, do której przypisujemy podsekcje\n",
    "current_section = None\n",
    "\n",
    "# Przetwarzamy każdą linię tekstu osobno\n",
    "for line in openai_result.split(\"\\n\"):  # Dzielimy cały tekst na linie i iterujemy przez każdą z nich\n",
    "    section_match = section_regex.match(line)  # Sprawdzamy, czy linia pasuje do wzorca sekcji\n",
    "    subsection_match = subsection_regex.match(line)  # Sprawdzamy, czy linia pasuje do wzorca podsekcji\n",
    "\n",
    "    if section_match:  # Jeśli linia to nowa sekcja\n",
    "        current_section = section_match.group(1)  # Pobieramy nazwę sekcji\n",
    "        result_dict[current_section] = []  # Dodajemy nową sekcję do słownika jako klucz z pustą listą podsekcji\n",
    "    elif subsection_match and current_section is not None:  # Jeśli linia to podsekcja i mamy aktywną sekcję\n",
    "        result_dict[current_section].append(subsection_match.group(1))  # Dodajemy podsekcję do odpowiedniej sekcji\n",
    "\n",
    "# Drukujemy wynikowy słownik zawierający sekcje i podsekcje\n",
    "print(result_dict)\n",
    "\n",
    "'''\n",
    "Opis działania kodu:\n",
    "\n",
    "Kod analizuje tekst konspektu artykułu i strukturyzuje go w postaci słownika, gdzie:\n",
    "\t•\tkluczem są główne sekcje (np. “Introduction”, “Efficient Data Management”, “Conclusion”),\n",
    "\t•\twartościami są listy podsekcji przypisanych do każdej sekcji.\n",
    "\n",
    "Zastosowanie kodu:\n",
    "\t•\tPrzetwarzanie i analiza tekstu – np. parsowanie konspektów, dokumentacji czy treści generowanych przez AI.\n",
    "\t•\tKonwersja tekstu do strukturyzowanej formy – np. JSON, baza danych, raporty.\n",
    "\t•\tAutomatyczne wydobywanie informacji – np. kategoryzowanie treści w aplikacjach AI lub NLP.\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f77320-aa3b-46f8-a93b-bde59c77dbd0",
   "metadata": {},
   "source": [
    "# Parsowanie dokumentu JSON za pomocą języka Python\n",
    "## three_json_parsing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d628ce-5e0a-4e7f-903e-f5de628cd19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Introduction': ['a. Overview of coding and programming languages', \"b. Importance of coding in today's technology-driven world\"], 'Conclusion': ['a. Recap of the benefits of learning code', 'b. The ongoing importance of coding skills in the modern world']}\n"
     ]
    }
   ],
   "source": [
    "import json  # Importujemy moduł `json`, który umożliwia pracę z danymi w formacie JSON w Pythonie.\n",
    "\n",
    "# Przykładowy wynik w formacie JSON, reprezentujący strukturę konspektu artykułu\n",
    "openai_json_result = \"\"\"\n",
    "{\n",
    "    \"Introduction\": [\n",
    "        \"a. Overview of coding and programming languages\",\n",
    "        \"b. Importance of coding in today's technology-driven world\"],\n",
    "    \"Conclusion\": [\n",
    "        \"a. Recap of the benefits of learning code\",\n",
    "        \"b. The ongoing importance of coding skills in the modern world\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Konwertujemy ciąg znaków w formacie JSON na obiekt Pythona (słownik)\n",
    "parsed_json_payload = json.loads(openai_json_result)\n",
    "\n",
    "# Drukujemy sparsowany obiekt JSON, który teraz ma postać słownika\n",
    "print(parsed_json_payload)\n",
    "\n",
    "'''\n",
    "Zastosowanie kodu:\n",
    "\t•\tParsowanie danych JSON – przydatne w pracy z danymi pochodzącymi z API, plików JSON czy odpowiedzi AI.\n",
    "\t•\tStrukturyzacja danych – umożliwia łatwą organizację i dostęp do informacji.\n",
    "\t•\tPrzetwarzanie tekstu – użyteczne np. do ekstrakcji informacji z generowanych treści i dalszej ich analizy.\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e583dd-235e-4ebc-9e46-4a1a52ed9eed",
   "metadata": {},
   "source": [
    "# Listing 3.3. Wykrywanie zdań za pomocą biblioteki spaCy\n",
    "\n",
    "Ten kod demonstruje podstawowe użycie biblioteki spaCy do podziału tekstu na zdania w języku polskim. Najpierw importuje bibliotekę spaCy i ładuje model językowy dla języka polskiego. Następnie definiuje przykładowy tekst, przetwarza go za pomocą modelu i w pętli wypisuje każde zdanie osobno.\n",
    "\n",
    "Cel użycia: Tego rodzaju kod można wykorzystać w aplikacjach przetwarzania języka naturalnego (NLP), takich jak analiza tekstu, automatyczne streszczanie, czy ekstrakcja informacji, gdzie potrzebny jest podział tekstu na mniejsze jednostki (zdania). Jest to szczególnie przydatne w analizie dużych ilości tekstu, np. w danych z mediów społecznościowych, artykułów czy książek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffafa9-343d-4356-8450-758f6bf3911c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85660f0a-f220-4907-8939-9d73a885ef0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'pl_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m  \u001b[38;5;66;03m# Importuje bibliotekę spaCy, która jest narzędziem do przetwarzania języka naturalnego (NLP), umożliwiającym analizę tekstu, taką jak podział na zdania czy rozpoznawanie części mowy.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpl_core_news_sm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tworzy obiekt nlp, ładując model językowy dla języka polskiego (\"pl_core_news_sm\" to mały model dla języka polskiego), który będzie używany do analizy tekstu.\u001b[39;00m\n\u001b[32m      5\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mTo jest zdanie. To jest inne zdanie.\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Definiuje zmienną tekstową 'text', która zawiera dwa przykładowe zdania oddzielone kropką, jako dane wejściowe do analizy.\u001b[39;00m\n\u001b[32m      7\u001b[39m doc = nlp(text)  \u001b[38;5;66;03m# Przetwarza tekst za pomocą modelu nlp, tworząc obiekt 'doc', który zawiera strukturalną reprezentację tekstu z informacjami o zdaniach, tokenach itp.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/Prompt-Engineering/venv/lib/python3.11/site-packages/spacy/__init__.py:51\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     28\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     29\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     35\u001b[39m ) -> Language:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/Prompt-Engineering/venv/lib/python3.11/site-packages/spacy/util.py:472\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'pl_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy  # Importuje bibliotekę spaCy, która jest narzędziem do przetwarzania języka naturalnego (NLP), umożliwiającym analizę tekstu, taką jak podział na zdania czy rozpoznawanie części mowy.\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_sm\")  # Tworzy obiekt nlp, ładując model językowy dla języka polskiego (\"pl_core_news_sm\" to mały model dla języka polskiego), który będzie używany do analizy tekstu.\n",
    "\n",
    "text = \"To jest zdanie. To jest inne zdanie.\"  # Definiuje zmienną tekstową 'text', która zawiera dwa przykładowe zdania oddzielone kropką, jako dane wejściowe do analizy.\n",
    "\n",
    "doc = nlp(text)  # Przetwarza tekst za pomocą modelu nlp, tworząc obiekt 'doc', który zawiera strukturalną reprezentację tekstu z informacjami o zdaniach, tokenach itp.\n",
    "\n",
    "for sent in doc.sents:  # Rozpoczyna pętlę, która iteruje przez wszystkie zdania w obiekcie 'doc'; 'doc.sents' to generator zwracający kolejne zdania tekstu.\n",
    "\n",
    "    print(sent.text)  # Wypisuje tekst każdego zdania; 'sent.text' zwraca ciąg znaków reprezentujący dane zdanie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e9c4f-d7e8-4ffc-9746-4c3ab5d001f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

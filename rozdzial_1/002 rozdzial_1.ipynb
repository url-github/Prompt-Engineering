{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb32a0f6-80aa-4934-9586-880c4a058268",
   "metadata": {},
   "source": [
    "# script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e97636-9a51-4c7f-823c-81e9f495615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  variant                                             prompt  \\\n",
      "0       A  Opis produktu: Buty, kt√≥re pasujƒÖ dla ka≈ºdego ...   \n",
      "1       A  Opis produktu: Buty, kt√≥re pasujƒÖ dla ka≈ºdego ...   \n",
      "2       A  Opis produktu: Buty, kt√≥re pasujƒÖ dla ka≈ºdego ...   \n",
      "3       A  Opis produktu: Buty, kt√≥re pasujƒÖ dla ka≈ºdego ...   \n",
      "4       A  Opis produktu: Buty, kt√≥re pasujƒÖ dla ka≈ºdego ...   \n",
      "5       B  Opis produktu: Automat do przygotowania mleczn...   \n",
      "6       B  Opis produktu: Automat do przygotowania mleczn...   \n",
      "7       B  Opis produktu: Automat do przygotowania mleczn...   \n",
      "8       B  Opis produktu: Automat do przygotowania mleczn...   \n",
      "9       B  Opis produktu: Automat do przygotowania mleczn...   \n",
      "\n",
      "                                            response  \n",
      "0  1. DostosowujƒÖce buty UniFit\\n2. Uniwersalne b...  \n",
      "1  1. UniSize Shoes\\n2. FlexFit Footwear\\n3. OneS...  \n",
      "2  1. UniFit Shoes\\n2. MultiSize Sneakers\\n3. Fle...  \n",
      "3  1. Dostosowane buty\\n2. Dopasowana obuwie\\n3. ...  \n",
      "4  1. Dostosowana doskona≈Ço≈õƒá\\n2. Uniwersalny kom...  \n",
      "5  UniFit Shoes, FlexSize Footwear, PerfectFit Fo...  \n",
      "6       SizeFlex, UniFit, StretchoShoes, PerfectFit.  \n",
      "7               FootFlex, SizeFit, AllFit, SoleMate.  \n",
      "8             FlexFit, OneSize, AdaptSole, ComfyFit.  \n",
      "9  FlexFit Shoe, UniversalSole, CustomStep, Adapt...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOpis dzia≈Çania kodu:\\nTen skrypt testuje dwa r√≥≈ºne warianty prompt√≥w do generowania nazw produkt√≥w przy u≈ºyciu modelu GPT-3.5-turbo.\\nNajpierw wczytywane sƒÖ zmienne ≈õrodowiskowe, w tym klucz OpenAI API. Nastƒôpnie definiowane sƒÖ dwa prompty - jeden \\nz minimalnƒÖ ilo≈õciƒÖ przyk≈Çad√≥w, a drugi z wiƒôkszƒÖ liczbƒÖ przyk≈Çad√≥w. Ka≈ºdy prompt jest testowany piƒôƒá razy \\n(z mo≈ºliwo≈õciƒÖ ≈Çatwego zwiƒôkszenia liczby test√≥w). Wyniki sƒÖ zbierane w obiekcie Pandas DataFrame i zapisywane \\ndo pliku CSV, co pozwala na p√≥≈∫niejszƒÖ analizƒô.\\n\\nZastosowanie:\\n- Por√≥wnanie skuteczno≈õci r√≥≈ºnych wariant√≥w prompt√≥w.\\n- Analiza jako≈õci generowanych odpowiedzi.\\n- Tworzenie benchmark√≥w dla r√≥≈ºnych modeli AI.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Wczytaj zmienne ≈õrodowiskowe z pliku .env, aby m√≥c korzystaƒá z ukrytych kluczy API.\n",
    "\n",
    "# Definicja dw√≥ch wariant√≥w prompt√≥w do testowania generowania nazw produkt√≥w\n",
    "prompt_A = \"\"\"Opis produktu: Buty, kt√≥re pasujƒÖ dla ka≈ºdego rozmiaru stopy.\n",
    "S≈Çowa poczƒÖtkowe: dostosowanie, dopasowanie, uniwersalny rozmiar\n",
    "Nazwy produkt√≥w:\"\"\"\n",
    "\n",
    "prompt_B = \"\"\"Opis produktu: Automat do przygotowania mlecznych koktajli w domu.\n",
    "S≈Çowa poczƒÖtkowe: szybki, zdrowy, kompaktowy.\n",
    "Nazwy produkt√≥w: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "Opis produktu: Zegarek, kt√≥ry podaje dok≈Çadny czas w przestrzeni kosmicznej\n",
    "S≈Çowa poczƒÖtkowe: astronauta, odporne na przestrze≈Ñ kosmicznƒÖ, orbita eliptyczna\n",
    "Nazwy produkt√≥w: AstroTime, SpaceGuard, Orbit-Accurate, EliptoTime.\n",
    "Opis produktu: Buty, kt√≥re pasujƒÖ dla ka≈ºdego rozmiaru stopy.\n",
    "S≈Çowa poczƒÖtkowe: dostosowanie, dopasowanie, uniwersalny rozmiar\n",
    "Nazwy produkt√≥w:\"\"\"\n",
    "\n",
    "# Lista testowanych prompt√≥w\n",
    "test_prompts = [prompt_A, prompt_B]\n",
    "\n",
    "import pandas as pd  # Biblioteka do obs≈Çugi danych tabelarycznych\n",
    "from openai import OpenAI  # Import klienta OpenAI do obs≈Çugi API\n",
    "import os  # Modu≈Ç do zarzƒÖdzania zmiennymi ≈õrodowiskowymi\n",
    "\n",
    "# Tworzenie klienta OpenAI z kluczem API przechowywanym w zmiennej ≈õrodowiskowej\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],  # Pobranie klucza API z systemowych zmiennych ≈õrodowiskowych\n",
    ")\n",
    "\n",
    "def get_response(prompt):\n",
    "    \"\"\"Funkcja do uzyskania odpowiedzi z modelu GPT na podany prompt.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Model u≈ºywany do generowania odpowiedzi\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Jeste≈õ pomocnym asystentem.\"},  # Kontekst dla modelu\n",
    "            {\"role\": \"user\", \"content\": prompt}  # W≈Ça≈õciwy prompt przekazany do modelu\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content  # Zwr√≥cenie tre≈õci pierwszej wygenerowanej odpowiedzi\n",
    "\n",
    "responses = []  # Lista na zebrane odpowiedzi\n",
    "num_tests = 5  # Liczba test√≥w dla ka≈ºdego prompta\n",
    "\n",
    "for idx, prompt in enumerate(test_prompts):  # Iteracja po promptach\n",
    "    var_name = chr(ord('A') + idx)  # Zamiana indeksu na literƒô (A, B, C...)\n",
    "    \n",
    "    for i in range(num_tests):  # Wykonanie test√≥w wielokrotnie dla lepszego por√≥wnania\n",
    "        response = get_response(prompt)  # Pobranie odpowiedzi od modelu\n",
    "        data = {\n",
    "            \"variant\": var_name,  # Oznaczenie wariantu (A/B)\n",
    "            \"prompt\": prompt,  # Tre≈õƒá prompta\n",
    "            \"response\": response  # Odpowied≈∫ modelu\n",
    "        }\n",
    "        responses.append(data)  # Dodanie wyniku do listy\n",
    "\n",
    "# Konwersja wynik√≥w na obiekt DataFrame (tabelƒô Pandas)\n",
    "df = pd.DataFrame(responses)\n",
    "\n",
    "# Zapis wynik√≥w do pliku CSV\n",
    "df.to_csv(\"odpowiedzi.csv\", index=False)\n",
    "\n",
    "# Wy≈õwietlenie tabeli w terminalu\n",
    "print(df)\n",
    "\n",
    "\"\"\"\n",
    "Opis dzia≈Çania kodu:\n",
    "Ten skrypt testuje dwa r√≥≈ºne warianty prompt√≥w do generowania nazw produkt√≥w przy u≈ºyciu modelu GPT-3.5-turbo.\n",
    "Najpierw wczytywane sƒÖ zmienne ≈õrodowiskowe, w tym klucz OpenAI API. Nastƒôpnie definiowane sƒÖ dwa prompty - jeden \n",
    "z minimalnƒÖ ilo≈õciƒÖ przyk≈Çad√≥w, a drugi z wiƒôkszƒÖ liczbƒÖ przyk≈Çad√≥w. Ka≈ºdy prompt jest testowany piƒôƒá razy \n",
    "(z mo≈ºliwo≈õciƒÖ ≈Çatwego zwiƒôkszenia liczby test√≥w). Wyniki sƒÖ zbierane w obiekcie Pandas DataFrame i zapisywane \n",
    "do pliku CSV, co pozwala na p√≥≈∫niejszƒÖ analizƒô.\n",
    "\n",
    "Zastosowanie:\n",
    "- Por√≥wnanie skuteczno≈õci r√≥≈ºnych wariant√≥w prompt√≥w.\n",
    "- Analiza jako≈õci generowanych odpowiedzi.\n",
    "- Tworzenie benchmark√≥w dla r√≥≈ºnych modeli AI.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087aa814-36ba-4119-9cde-0ddda979ecf5",
   "metadata": {},
   "source": [
    "# script2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c01ab3-d1b6-4551-91ee-1a6e6285bdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9465d16653024a5cb799b2ed0b9d4618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p>1. UniSize Shoes\\n2. FlexFit Footwear\\n3. OneSize Fits All Boots\\n4. AdaptiFit Sneakers\\n5. Uni‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbb28a301bc4212a83bd653dcee468b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='üëé', style=ButtonStyle()), Button(description='üëç', style=ButtonStyle())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efecc34973d546b7919bda15bef22f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Odpowied≈∫: 1/10')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nOpis dzia≈Çania kodu:\\nTen skrypt implementuje system test√≥w A/B dla odpowiedzi generowanych przez model LLM.\\nPozwala u≈ºytkownikowi oceniaƒá odpowiedzi poprzez klikniƒôcie kciuka w g√≥rƒô (pozytywna ocena) lub kciuka w d√≥≈Ç (negatywna ocena).\\nPo ocenie wszystkich odpowiedzi wyniki sƒÖ zapisywane do pliku \"results.csv\",\\na podsumowanie wy≈õwietlane jest w konsoli. \\n\\nZastosowanie:\\n- Testowanie r√≥≈ºnych wariant√≥w prompt√≥w i ich wp≈Çywu na odpowiedzi modelu\\n- Por√≥wnywanie jako≈õci odpowiedzi w zale≈ºno≈õci od wersji modelu lub parametr√≥w\\n- Uproszczenie procesu zbierania opinii u≈ºytkownik√≥w na temat generowanych tre≈õci\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Wczytaj zmienne ≈õrodowiskowe z pliku .env, np. klucze API\n",
    "\n",
    "import ipywidgets as widgets  # Biblioteka do tworzenia interaktywnych wid≈ºet√≥w w Jupyter Notebook\n",
    "from IPython.display import display  # Funkcja do wy≈õwietlania element√≥w interfejsu\n",
    "import pandas as pd  # Biblioteka do pracy z danymi w postaci tabelarycznej (ramki danych Pandas)\n",
    "\n",
    "# Wczytaj dane z pliku CSV zawierajƒÖcego odpowiedzi\n",
    "# Plik \"odpowiedzi.csv\" powinien zawieraƒá wygenerowane odpowiedzi dla test√≥w A/B\n",
    "# Ka≈ºdy wiersz to jedna odpowied≈∫ modelu LLM w danym wariancie testowym\n",
    "\n",
    "df = pd.read_csv(\"odpowiedzi.csv\") \n",
    "\n",
    "# Wymieszaj dane, aby u≈ºytkownik nie ocenia≈Ç odpowiedzi w okre≈õlonej kolejno≈õci\n",
    "# `frac=1` oznacza, ≈ºe bierzemy 100% danych i je mieszamy\n",
    "# `reset_index(drop=True)` resetuje indeksy po przetasowaniu\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Zmienna przechowujƒÖca numer aktualnie ocenianej odpowiedzi\n",
    "response_index = 0  # Start od pierwszego rekordu\n",
    "\n",
    "# Dodaj nowƒÖ kolumnƒô do ramki danych, w kt√≥rej bƒôdziemy przechowywaƒá informacje zwrotne\n",
    "# Typ `str` oznacza, ≈ºe warto≈õci w tej kolumnie bƒôdƒÖ tekstowe (choƒá zapisujemy liczby 0 i 1)\n",
    "df['feedback'] = pd.Series(dtype='str')\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    \"\"\"\n",
    "    Funkcja wywo≈Çywana po klikniƒôciu przycisku \"kciuk w g√≥rƒô\" lub \"kciuk w d√≥≈Ç\".\n",
    "    Aktualizuje ocenƒô odpowiedzi i przechodzi do kolejnej.\n",
    "    \"\"\"\n",
    "    global response_index  # Odwo≈Çanie do zmiennej globalnej przechowujƒÖcej indeks\n",
    "    \n",
    "    # Przypisz warto≈õƒá 1 dla kciuka w g√≥rƒô, 0 dla kciuka w d√≥≈Ç\n",
    "    user_feedback = 1 if b.description == \"\\U0001F44D\" else 0  \n",
    "    \n",
    "    # Zaktualizuj kolumnƒô \"feedback\" w danym wierszu\n",
    "    df.at[response_index, 'feedback'] = user_feedback\n",
    "    \n",
    "    # Przejd≈∫ do nastƒôpnej odpowiedzi\n",
    "    response_index += 1\n",
    "    \n",
    "    # Je≈õli nadal sƒÖ odpowiedzi do oceny, wy≈õwietl kolejnƒÖ\n",
    "    if response_index < len(df):\n",
    "        update_response()\n",
    "    else:\n",
    "        # Po zako≈Ñczeniu oceniania zapisujemy dane do pliku \"results.csv\"\n",
    "        df.to_csv(\"results.csv\", index=False)\n",
    "        print(\"Zako≈Ñczono testy A/B. Oto wyniki:\")\n",
    "        \n",
    "        # Podsumowanie wynik√≥w: liczba ocen i ≈õrednia ocena dla ka≈ºdego wariantu\n",
    "        summary_df = df.groupby('variant').agg(\n",
    "            count=('feedback', 'count'),  # Liczba ocenionych odpowiedzi\n",
    "            score=('feedback', 'mean')   # ≈öredni wynik (0-1) dla ka≈ºdego wariantu\n",
    "        ).reset_index()\n",
    "        print(summary_df)  # Wy≈õwietl podsumowanie\n",
    "\n",
    "def update_response():\n",
    "    \"\"\"\n",
    "    Funkcja aktualizujƒÖca wy≈õwietlanƒÖ odpowied≈∫ w wid≈ºecie HTML.\n",
    "    \"\"\"\n",
    "    new_response = df.iloc[response_index]['response']  # Pobierz aktualnƒÖ odpowied≈∫\n",
    "    \n",
    "    # Je≈õli odpowied≈∫ istnieje, sformatuj jƒÖ jako HTML\n",
    "    if pd.notna(new_response):\n",
    "        new_response = \"<p>\" + new_response + \"</p>\"\n",
    "    else:\n",
    "        new_response = \"<p>Brak odpowiedzi</p>\"\n",
    "    \n",
    "    response.value = new_response  # Zaktualizuj tre≈õƒá wy≈õwietlanej odpowiedzi\n",
    "    count_label.value = f\"Odpowied≈∫: {response_index + 1}/{len(df)}\"  # Wy≈õwietl liczbƒô ocenionych odpowiedzi\n",
    "\n",
    "# Utworzenie interfejsu u≈ºytkownika\n",
    "response = widgets.HTML()  # Wid≈ºet do wy≈õwietlania tre≈õci odpowiedzi\n",
    "count_label = widgets.Label()  # Wid≈ºet do wy≈õwietlania numeru odpowiedzi\n",
    "update_response()  # Zainicjalizowanie pierwszej odpowiedzi\n",
    "\n",
    "# Utworzenie przycisk√≥w do oceny odpowiedzi\n",
    "thumbs_up_button = widgets.Button(description='\\U0001F44D')  # Kciuk w g√≥rƒô\n",
    "thumbs_up_button.on_click(on_button_clicked)  # Po klikniƒôciu wywo≈Çuje funkcjƒô oceny\n",
    "\n",
    "thumbs_down_button = widgets.Button(description='\\U0001F44E')  # Kciuk w d√≥≈Ç\n",
    "thumbs_down_button.on_click(on_button_clicked)  # Po klikniƒôciu wywo≈Çuje funkcjƒô oceny\n",
    "\n",
    "# Rozmieszczenie przycisk√≥w w poziomie\n",
    "button_box = widgets.HBox([thumbs_down_button, thumbs_up_button])\n",
    "\n",
    "# Wy≈õwietlenie interfejsu w Jupyter Notebook\n",
    "display(response, button_box, count_label)\n",
    "\n",
    "\"\"\"\n",
    "Opis dzia≈Çania kodu:\n",
    "Ten skrypt implementuje system test√≥w A/B dla odpowiedzi generowanych przez model LLM.\n",
    "Pozwala u≈ºytkownikowi oceniaƒá odpowiedzi poprzez klikniƒôcie kciuka w g√≥rƒô (pozytywna ocena) lub kciuka w d√≥≈Ç (negatywna ocena).\n",
    "Po ocenie wszystkich odpowiedzi wyniki sƒÖ zapisywane do pliku \"results.csv\",\n",
    "a podsumowanie wy≈õwietlane jest w konsoli. \n",
    "\n",
    "Zastosowanie:\n",
    "- Testowanie r√≥≈ºnych wariant√≥w prompt√≥w i ich wp≈Çywu na odpowiedzi modelu\n",
    "- Por√≥wnywanie jako≈õci odpowiedzi w zale≈ºno≈õci od wersji modelu lub parametr√≥w\n",
    "- Uproszczenie procesu zbierania opinii u≈ºytkownik√≥w na temat generowanych tre≈õci\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

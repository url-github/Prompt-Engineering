{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d461dbb-7440-4bbf-9e40-77219ec5a0cc",
   "metadata": {},
   "source": [
    "# Generator dowcipów z wykorzystaniem frameworka LangChain\n",
    "\n",
    "Ten kod wykorzystuje bibliotekę LangChain do interakcji z modelem językowym OpenAI (np. GPT-3.5 lub GPT-4). Najpierw konfiguruje instancję modelu z kluczem API i ustala poziom kreatywności odpowiedzi za pomocą parametru temperature. Następnie definiuje kontekst rozmowy (model jako starszy inżynier oprogramowania) i zadaje pytanie o żart. Kod wywołuje model w dwóch trybach:\n",
    "\n",
    "\n",
    "Pojedyncze wywołanie (metoda invoke): zwraca jedną odpowiedź, np. żart.\n",
    "Wsadowe wywołanie (metoda batch): wysyła dwa identyczne zapytania jednocześnie i zwraca listę odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f5f5ea-efeb-45a3-ab80-11f1aac968e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce699494-2003-4fa9-8891-526ab11774eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oczywiście! Oto jeden z moich ulubionych żartów:\n",
      "\n",
      "Dlaczego inżynierowie oprogramowania lubią spacerować po plaży?\n",
      "\n",
      "Bo nawet na piasku szukają błędów!\n",
      "[AIMessage(content='Oczywiście! Oto jeden z moich ulubionych żartów o inżynierach oprogramowania:\\n\\nDlaczego inżynierowie oprogramowania nie mogą zjeść obiadu?\\n\\nBo zawsze mają za mało RAMu!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 54, 'total_tokens': 116, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a6995a83-c358-4834-b24c-9c688a00062e-0', usage_metadata={'input_tokens': 54, 'output_tokens': 62, 'total_tokens': 116, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Oczywiście! Oto jeden z moich ulubionych żartów o inżynierach oprogramowania:\\n\\nDlaczego inżynierowie oprogramowania nie lubią spotykać się na plaży?\\n\\nBo zawsze mają problem z debugowaniem!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 54, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d3ef4264-fac5-4363-8a5b-ddd8de2cc464-0', usage_metadata={'input_tokens': 54, 'output_tokens': 64, 'total_tokens': 118, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.5)\n",
    "messages = [SystemMessage(content='''Jesteś starszym inżynierem oprogramowania w firmie typu startup.'''),\n",
    "HumanMessage(content='''Czy możesz przedstawić zabawny żart o inżynierach oprogramowania?''')]\n",
    "response = chat.invoke(input=messages)\n",
    "print(response.content)\n",
    "synchronous_llm_result = chat.batch([messages]*2)\n",
    "print(synchronous_llm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a6cc304-c0f4-4fcc-9dc8-f44b598ed35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treść odpowiedzi: Oczywiście! Oto jeden z moich ulubionych żartów o inżynierach oprogramowania:\n",
      "\n",
      "Dlaczego inżynierowie oprogramowania nie lubią spotykać się na plaży?\n",
      "\n",
      "Bo zawsze mają problem z zainstalowaniem \"plażowego trybu offline\"!\n",
      "Użyty model: gpt-3.5-turbo-0125\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(input=messages)\n",
    "print(f\"Treść odpowiedzi: {response.content}\")\n",
    "print(f\"Użyty model: {response.response_metadata['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447d8859-389e-4080-aee8-9d17570e2bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź 1: Oczywiście! Oto jeden z moich ulubionych żartów o inżynierach oprogramowania:\n",
      "\n",
      "Dlaczego inżynierowie oprogramowania nie lubią spotykać się na plaży?\n",
      "\n",
      "Bo zawsze mają problem z debugowaniem!\n",
      "Użyty model: gpt-3.5-turbo-0125\n",
      "Odpowiedź 2: Oczywiście! Oto jeden z moich ulubionych żartów:\n",
      "\n",
      "Dlaczego inżynierowie oprogramowania zawsze myślą pozytywnie?\n",
      "\n",
      "Bo w ich kodzie nie ma miejsca na negatywne myślenie!\n",
      "Użyty model: gpt-3.5-turbo-0125\n"
     ]
    }
   ],
   "source": [
    "synchronous_llm_result = chat.batch([messages]*2)\n",
    "for idx, result in enumerate(synchronous_llm_result):\n",
    "    print(f\"Odpowiedź {idx+1}: {result.content}\")\n",
    "    print(f\"Użyty model: {result.response_metadata['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f00d0-fb66-4ac1-b1c6-072f2e4c3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importujemy klasę ChatOpenAI z modułu langchain_openai.chat_models; ta klasa pozwala na komunikację z modelem OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Importujemy klasy AIMessage, HumanMessage i SystemMessage z modułu langchain.schema; służą one do definiowania różnych typów wiadomości w rozmowie z modelem\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Tworzymy instancję klasy ChatOpenAI, przekazując klucz API jako \"OPENAI_API_KEY\"; ta linia inicjuje połączenie z modelem OpenAI\n",
    "# Uwaga: \"OPENAI_API_KEY\" to placeholder - w praktyce należy użyć prawdziwego klucza API lub wczytać go np. z pliku .env\n",
    "chat = ChatOpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "\n",
    "# Nadpisujemy poprzednią instancję chatu, tworząc nową z parametrem temperature=0.5; temperature określa losowość odpowiedzi modelu\n",
    "# Wartość 0.5 oznacza umiarkowaną kreatywność: 0 to odpowiedzi przewidywalne, 1 to bardziej losowe i kreatywne\n",
    "chat = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# Definiujemy listę wiadomości, która zawiera dwa obiekty:\n",
    "# - SystemMessage: ustala kontekst rozmowy, informując model, że jest starszym inżynierem w startupie; to \"instrukcja systemowa\" dla modelu\n",
    "# - HumanMessage: reprezentuje zapytanie użytkownika, w tym przypadku prośbę o żart o inżynierach oprogramowania\n",
    "messages = [\n",
    "    SystemMessage(content='''Jesteś starszym inżynierem oprogramowania w firmie typu startup.'''),\n",
    "    HumanMessage(content='''Czy możesz przedstawić zabawny żart o inżynierach oprogramowania?''')\n",
    "]\n",
    "\n",
    "# Wywołujemy metodę invoke na obiekcie chat, przekazując listę wiadomości jako argument; metoda ta wysyła zapytanie do modelu i zwraca odpowiedź\n",
    "# Odpowiedź jest zwracana jako obiekt AIMessage, który zawiera treść wygenerowaną przez model\n",
    "response = chat.invoke(input=messages)\n",
    "\n",
    "# Wyświetlamy treść odpowiedzi modelu, dostępną w atrybucie content obiektu response; to będzie np. żart wygenerowany przez model\n",
    "print(response.content)\n",
    "\n",
    "# Używamy metody batch, aby synchronicznie wysłać dwa identyczne zapytania do modelu; [messages]*2 tworzy listę z dwoma kopiami messages\n",
    "# Metoda batch jest bardziej efektywna niż wielokrotne wywoływanie invoke, gdy potrzebujemy wielu odpowiedzi naraz\n",
    "synchronous_llm_result = chat.batch([messages]*2)\n",
    "\n",
    "# Wyświetlamy wynik metody batch, który jest listą obiektów AIMessage; każda odpowiedź zawiera treść wygenerowaną dla danego zapytania\n",
    "print(synchronous_llm_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

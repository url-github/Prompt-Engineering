{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d461dbb-7440-4bbf-9e40-77219ec5a0cc",
   "metadata": {},
   "source": [
    "# Generator dowcipów z wykorzystaniem frameworka LangChain\n",
    "\n",
    "Ten kod wykorzystuje bibliotekę LangChain do interakcji z modelem językowym OpenAI (np. GPT-3.5 lub GPT-4). Najpierw konfiguruje instancję modelu z kluczem API i ustala poziom kreatywności odpowiedzi za pomocą parametru temperature. Następnie definiuje kontekst rozmowy (model jako starszy inżynier oprogramowania) i zadaje pytanie o żart. Kod wywołuje model w dwóch trybach:\n",
    "\n",
    "\n",
    "Pojedyncze wywołanie (metoda invoke): zwraca jedną odpowiedź, np. żart.\n",
    "Wsadowe wywołanie (metoda batch): wysyła dwa identyczne zapytania jednocześnie i zwraca listę odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f5f5ea-efeb-45a3-ab80-11f1aac968e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce699494-2003-4fa9-8891-526ab11774eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oczywiście! Oto jeden z moich ulubionych żartów:\n",
      "\n",
      "Dlaczego inżynierowie oprogramowania lubią spacerować po plaży?\n",
      "\n",
      "Bo nawet na piasku szukają błędów!\n",
      "[AIMessage(content='Oczywiście! Oto jeden z moich ulubionych żartów o inżynierach oprogramowania:\\n\\nDlaczego inżynierowie oprogramowania nie mogą zjeść obiadu?\\n\\nBo zawsze mają za mało RAMu!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 54, 'total_tokens': 116, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a6995a83-c358-4834-b24c-9c688a00062e-0', usage_metadata={'input_tokens': 54, 'output_tokens': 62, 'total_tokens': 116, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Oczywiście! Oto jeden z moich ulubionych żartów o inżynierach oprogramowania:\\n\\nDlaczego inżynierowie oprogramowania nie lubią spotykać się na plaży?\\n\\nBo zawsze mają problem z debugowaniem!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 54, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d3ef4264-fac5-4363-8a5b-ddd8de2cc464-0', usage_metadata={'input_tokens': 54, 'output_tokens': 64, 'total_tokens': 118, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.5)\n",
    "messages = [SystemMessage(content='''Jesteś starszym inżynierem oprogramowania w firmie typu startup.'''),\n",
    "HumanMessage(content='''Czy możesz przedstawić zabawny żart o inżynierach oprogramowania?''')]\n",
    "response = chat.invoke(input=messages)\n",
    "print(response.content)\n",
    "synchronous_llm_result = chat.batch([messages]*2)\n",
    "print(synchronous_llm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a6cc304-c0f4-4fcc-9dc8-f44b598ed35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treść odpowiedzi: Oczywiście! Oto jeden z moich ulubionych żartów o inżynierach oprogramowania:\n",
      "\n",
      "Dlaczego inżynierowie oprogramowania nie lubią spotykać się na plaży?\n",
      "\n",
      "Bo zawsze mają problem z zainstalowaniem \"plażowego trybu offline\"!\n",
      "Użyty model: gpt-3.5-turbo-0125\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(input=messages)\n",
    "print(f\"Treść odpowiedzi: {response.content}\")\n",
    "print(f\"Użyty model: {response.response_metadata['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447d8859-389e-4080-aee8-9d17570e2bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odpowiedź 1: Oczywiście! Oto jeden z moich ulubionych żartów o inżynierach oprogramowania:\n",
      "\n",
      "Dlaczego inżynierowie oprogramowania nie lubią spotykać się na plaży?\n",
      "\n",
      "Bo zawsze mają problem z debugowaniem!\n",
      "Użyty model: gpt-3.5-turbo-0125\n",
      "Odpowiedź 2: Oczywiście! Oto jeden z moich ulubionych żartów:\n",
      "\n",
      "Dlaczego inżynierowie oprogramowania zawsze myślą pozytywnie?\n",
      "\n",
      "Bo w ich kodzie nie ma miejsca na negatywne myślenie!\n",
      "Użyty model: gpt-3.5-turbo-0125\n"
     ]
    }
   ],
   "source": [
    "synchronous_llm_result = chat.batch([messages]*2)\n",
    "for idx, result in enumerate(synchronous_llm_result):\n",
    "    print(f\"Odpowiedź {idx+1}: {result.content}\")\n",
    "    print(f\"Użyty model: {result.response_metadata['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f00d0-fb66-4ac1-b1c6-072f2e4c3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importujemy klasę ChatOpenAI z modułu langchain_openai.chat_models; ta klasa pozwala na komunikację z modelem OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Importujemy klasy AIMessage, HumanMessage i SystemMessage z modułu langchain.schema; służą one do definiowania różnych typów wiadomości \n",
    "# w rozmowie z modelem\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Tworzymy instancję klasy ChatOpenAI, przekazując klucz API jako \"OPENAI_API_KEY\"; ta linia inicjuje połączenie z modelem OpenAI\n",
    "# Uwaga: \"OPENAI_API_KEY\" to placeholder - w praktyce należy użyć prawdziwego klucza API lub wczytać go np. z pliku .env\n",
    "chat = ChatOpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "\n",
    "# Nadpisujemy poprzednią instancję chatu, tworząc nową z parametrem temperature=0.5; temperature określa losowość odpowiedzi modelu\n",
    "# Wartość 0.5 oznacza umiarkowaną kreatywność: 0 to odpowiedzi przewidywalne, 1 to bardziej losowe i kreatywne\n",
    "chat = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# Definiujemy listę wiadomości, która zawiera dwa obiekty:\n",
    "# - SystemMessage: ustala kontekst rozmowy, informując model, że jest starszym inżynierem w startupie; to \"instrukcja systemowa\" dla modelu\n",
    "# - HumanMessage: reprezentuje zapytanie użytkownika, w tym przypadku prośbę o żart o inżynierach oprogramowania\n",
    "messages = [\n",
    "    SystemMessage(content='''Jesteś starszym inżynierem oprogramowania w firmie typu startup.'''),\n",
    "    HumanMessage(content='''Czy możesz przedstawić zabawny żart o inżynierach oprogramowania?''')\n",
    "]\n",
    "\n",
    "# Wywołujemy metodę invoke na obiekcie chat, przekazując listę wiadomości jako argument; metoda ta wysyła zapytanie do modelu i zwraca odpowiedź\n",
    "# Odpowiedź jest zwracana jako obiekt AIMessage, który zawiera treść wygenerowaną przez model\n",
    "response = chat.invoke(input=messages)\n",
    "\n",
    "# Wyświetlamy treść odpowiedzi modelu, dostępną w atrybucie content obiektu response; to będzie np. żart wygenerowany przez model\n",
    "print(response.content)\n",
    "\n",
    "# Używamy metody batch, aby synchronicznie wysłać dwa identyczne zapytania do modelu; [messages]*2 tworzy listę z dwoma kopiami messages\n",
    "# Metoda batch jest bardziej efektywna niż wielokrotne wywoływanie invoke, gdy potrzebujemy wielu odpowiedzi naraz\n",
    "synchronous_llm_result = chat.batch([messages]*2)\n",
    "\n",
    "# Wyświetlamy wynik metody batch, który jest listą obiektów AIMessage; każda odpowiedź zawiera treść wygenerowaną dla danego zapytania\n",
    "print(synchronous_llm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2646d1-ebb0-48ca-a7ef-73d329d8715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMessage: Obiekt reprezentujący odpowiedź modelu, zawiera treść i metadane\n",
    "AIMessage(\n",
    "    # content: Treść wygenerowana przez model, np. żart o inżynierach oprogramowania\n",
    "    content='Oczywiście! Oto jeden z moich ulubionych żartów o inżynierach oprogramowania:\\n\\nDlaczego inżynierowie oprogramowania nie mogą zjeść obiadu?\\n\\nBo zawsze mają za mało RAMu!',\n",
    "    \n",
    "    # additional_kwargs: Dodatkowe argumenty związane z odpowiedzią\n",
    "    additional_kwargs={\n",
    "        # refusal: Informacja o odmowie odpowiedzi (None - brak odmowy, model odpowiedział poprawnie)\n",
    "        'refusal': None\n",
    "    },\n",
    "    \n",
    "    # response_metadata: Słownik z szczegółowymi metadanymi o procesie generowania odpowiedzi\n",
    "    response_metadata={\n",
    "        # token_usage: Dane o zużyciu tokenów podczas generowania odpowiedzi\n",
    "        'token_usage': {\n",
    "            # completion_tokens: Liczba tokenów w wygenerowanej odpowiedzi (tu 62)\n",
    "            'completion_tokens': 62,\n",
    "            # prompt_tokens: Liczba tokenów w zapytaniu użytkownika (tu 54)\n",
    "            'prompt_tokens': 54,\n",
    "            # total_tokens: Suma tokenów zapytania i odpowiedzi (tu 116)\n",
    "            'total_tokens': 116,\n",
    "            # completion_tokens_details: Szczegóły dotyczące tokenów odpowiedzi\n",
    "            'completion_tokens_details': {\n",
    "                # accepted_prediction_tokens: Tokeny zaakceptowane przez predykcję modelu (tu 0, brak predykcji)\n",
    "                'accepted_prediction_tokens': 0,\n",
    "                # audio_tokens: Tokeny związane z audio w odpowiedzi (tu 0, brak audio)\n",
    "                'audio_tokens': 0,\n",
    "                # reasoning_tokens: Tokeny użyte do \"myślenia\" modelu (tu 0, brak dedykowanego myślenia)\n",
    "                'reasoning_tokens': 0,\n",
    "                # rejected_prediction_tokens: Tokeny odrzucone przez predykcję (tu 0, brak odrzuconych)\n",
    "                'rejected_prediction_tokens': 0\n",
    "            },\n",
    "            # prompt_tokens_details: Szczegóły dotyczące tokenów zapytania\n",
    "            'prompt_tokens_details': {\n",
    "                # audio_tokens: Tokeny audio w zapytaniu (tu 0, brak audio)\n",
    "                'audio_tokens': 0,\n",
    "                # cached_tokens: Tokeny odczytane z pamięci podręcznej (tu 0, brak cache)\n",
    "                'cached_tokens': 0\n",
    "            }\n",
    "        },\n",
    "        # model_name: Nazwa modelu użytego do generowania odpowiedzi (tu 'gpt-3.5-turbo-0125')\n",
    "        'model_name': 'gpt-3.5-turbo-0125',\n",
    "        # system_fingerprint: Unikalny identyfikator wersji modelu (tu None, brak danych)\n",
    "        'system_fingerprint': None,\n",
    "        # finish_reason: Powód zakończenia generowania odpowiedzi (tu 'stop' - model zakończył naturalnie)\n",
    "        'finish_reason': 'stop',\n",
    "        # logprobs: Logarytmy prawdopodobieństw tokenów (tu None, nie zażądano)\n",
    "        'logprobs': None\n",
    "    },\n",
    "    \n",
    "    # id: Unikalny identyfikator tej konkretnej odpowiedzi (np. 'run-a6995a83-c358-4834-b24c-9c688a00062e-0')\n",
    "    id='run-a6995a83-c358-4834-b24c-9c688a00062e-0',\n",
    "    \n",
    "    # usage_metadata: Uproszczone podsumowanie użycia tokenów\n",
    "    usage_metadata={\n",
    "        # input_tokens: Liczba tokenów w zapytaniu (tu 54)\n",
    "        'input_tokens': 54,\n",
    "        # output_tokens: Liczba tokenów w odpowiedzi (tu 62)\n",
    "        'output_tokens': 62,\n",
    "        # total_tokens: Suma tokenów zapytania i odpowiedzi (tu 116)\n",
    "        'total_tokens': 116,\n",
    "        # input_token_details: Szczegóły tokenów zapytania\n",
    "        'input_token_details': {\n",
    "            # audio: Tokeny audio w zapytaniu (tu 0, brak audio)\n",
    "            'audio': 0,\n",
    "            # cache_read: Tokeny odczytane z pamięci podręcznej (tu 0, brak cache)\n",
    "            'cache_read': 0\n",
    "        },\n",
    "        # output_token_details: Szczegóły tokenów odpowiedzi\n",
    "        'output_token_details': {\n",
    "            # audio: Tokeny audio w odpowiedzi (tu 0, brak audio)\n",
    "            'audio': 0,\n",
    "            # reasoning: Tokeny użyte do \"myślenia\" (tu 0, brak dedykowanego myślenia)\n",
    "            'reasoning': 0\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea9b260-6058-4924-a429-8a8247f50b7c",
   "metadata": {},
   "source": [
    "# Język wyrażeń LangChain (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c56bbe34-e191-4a48-b262-470c8e2937d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(api_key=\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a908f2b2-54d3-4a39-9794-80c159f9930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. DataMedAI\n",
      "2. SummarizeHealth\n",
      "3. SmartChart AI\n",
      "4. MedSynth\n",
      "5. DocSummarize\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import (SystemMessagePromptTemplate, ChatPromptTemplate)\n",
    "\n",
    "template = \"\"\"\n",
    "Jesteś kreatywnym konsultantem, który wymyśla nazwy firm.\n",
    "Musisz spełniać następujące reguły: \n",
    "{principles}\n",
    "Wygeneruj listę numerowaną pięciu chwytliwych nazw dla startupu w branży {industry}, które muszą radzić sobie z {context}?\n",
    "Oto przykład formatu danych: \n",
    "1. Nazwa1\n",
    "2. Nazwa2\n",
    "3. Nazwa3\n",
    "4. Nazwa4\n",
    "5. Nazwa5\n",
    "\"\"\"\n",
    "model = ChatOpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt])\n",
    "chain = chat_prompt | model\n",
    "result = chain.invoke({\n",
    "    \"industry\": \"medyczna\",\n",
    "    \"context\":'''tworzenie rozwiązań AI związanych z automatycznym podsumowywaniem danych pacjentów''',\n",
    "    \"principles\":'''1. Każda nazwa powinna być krótka i łatwa do zapamiętania. 2. Każda nazwa powinna być łatwa do wymówienia. 3. Każda nazwa powinna być unikatowa i nie być zajęta przez inną firmę.'''\n",
    "})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79a28e81-3db8-4b1f-a8f8-8fd0d3ce1e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. MedAIsum\n",
      "2. DocuBotix\n",
      "3. DataMedSum\n",
      "4. IntelliChart\n",
      "5. MediSyncAI\n"
     ]
    }
   ],
   "source": [
    "# Importowanie klasy ChatOpenAI z modułu langchain_openai.chat_models - ta klasa służy do tworzenia instancji modelu czatu OpenAI, \n",
    "# który generuje odpowiedzi\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Importowanie klas SystemMessagePromptTemplate i ChatPromptTemplate z modułu langchain_core.prompts - \n",
    "# te klasy są używane do tworzenia szablonów promptów, które określają, jak model ma odpowiadać\n",
    "from langchain_core.prompts import (SystemMessagePromptTemplate, ChatPromptTemplate)\n",
    "\n",
    "# Definiowanie szablonu tekstowego dla promptu - zawiera instrukcje dla modelu i miejsca na zmienne: \n",
    "# principles, industry, context; szablon określa zadanie generowania nazw firm\n",
    "template = \"\"\"\n",
    "Jesteś kreatywnym konsultantem, który wymyśla nazwy firm.\n",
    "Musisz spełniać następujące reguły: \n",
    "{principles}\n",
    "Wygeneruj listę numerowaną pięciu chwytliwych nazw dla startupu w branży {industry}, które muszą radzić sobie z {context}?\n",
    "Oto przykład formatu danych: \n",
    "1. Nazwa1\n",
    "2. Nazwa2\n",
    "3. Nazwa3\n",
    "4. Nazwa4\n",
    "5. Nazwa5\n",
    "\"\"\"\n",
    "\n",
    "# Tworzenie instancji modelu ChatOpenAI - ten obiekt będzie używany do generowania odpowiedzi na podstawie podanego promptu\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Tworzenie szablonu promptu systemowego na podstawie zdefiniowanego szablonu - SystemMessagePromptTemplate \n",
    "# przekształca szablon w wiadomość systemową dla modelu\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "# Tworzenie obiektu ChatPromptTemplate z wiadomością systemową - ten obiekt formatuje prompt, który zostanie przekazany do modelu czatu\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt])\n",
    "\n",
    "# Tworzenie łańcucha (chain) łączącego prompt z modelem - operator | przekazuje sformatowany prompt do modelu, aby wygenerować odpowiedź\n",
    "chain = chat_prompt | model\n",
    "\n",
    "# Wywoływanie łańcucha z konkretnymi wartościami dla zmiennych w szablonie - \n",
    "# result przechowuje wygenerowaną odpowiedź modelu dla branży medycznej, kontekstu AI i zasad nazw\n",
    "result = chain.invoke({\n",
    "    \"industry\": \"medyczna\",\n",
    "    \"context\":'''tworzenie rozwiązań AI związanych z automatycznym podsumowywaniem danych pacjentów''',\n",
    "    \"principles\":'''1. Każda nazwa powinna być krótka i łatwa do zapamiętania. 2. Każda nazwa powinna być łatwa do wymówienia. 3. Każda nazwa powinna być unikatowa i nie być zajęta przez inną firmę.'''\n",
    "})\n",
    "\n",
    "# Drukowanie treści wyniku - wyświetla listę pięciu nazw wygenerowanych przez model w formacie zgodnym z szablonem\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e124a56-e540-423e-98f0-f580719e365b",
   "metadata": {},
   "source": [
    "# Stosowanie PromptTemplate z modelami czatowymi\n",
    "\n",
    "Kod ten służy do generowania wiadomości systemowej dla modelu LLM (np. GPT-4) w celu przetłumaczenia tekstu\n",
    "z jednego języka na drugi. Wykorzystuje on bibliotekę LangChain do łatwego zarządzania promptami\n",
    "oraz umożliwia interakcję z modelem OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d821f7f-c1dd-4a0a-92ba-c67173ca9be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I am a helpful assistant who translates Polish into English.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 34, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5e765e17-54de-4f6a-833e-bd2d2c284344-0' usage_metadata={'input_tokens': 34, 'output_tokens': 12, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate \n",
    "from langchain_openai.chat_models import ChatOpenAI \n",
    "\n",
    "prompt=PromptTemplate(\n",
    " template='''Jesteś pomocnym asystentem, który tłumaczy język {input_language} na {output_language}.''',\n",
    " input_variables=[\"input_language\", \"output_language\"],\n",
    ")\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\n",
    "chat = ChatOpenAI()\n",
    "print(chat.invoke(system_message_prompt.format_messages(\n",
    "input_language=\"polski\", output_language=\"angielski\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c37cef-dd9b-48e6-a9ce-0cc61ab681a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Główna treść odpowiedzi modelu - przetłumaczony tekst z angielskiego na polski.\n",
    "content='Jestem pomocnym asystentem, który tłumaczy z polskiego na angielski.' \n",
    "\n",
    "# Dodatkowe argumenty - w tym przypadku 'refusal' jest ustawione na None, co oznacza, że model nie odmówił odpowiedzi.\n",
    "additional_kwargs={'refusal': None} \n",
    "\n",
    "# Metadane odpowiedzi - zawiera informacje techniczne o procesie generowania odpowiedzi, takie jak zużycie tokenów czy model.\n",
    "response_metadata={\n",
    "    'token_usage': { \n",
    "        'completion_tokens': 12,  # Liczba tokenów w wygenerowanej odpowiedzi.\n",
    "        'prompt_tokens': 34,      # Liczba tokenów w zapytaniu wejściowym.\n",
    "        'total_tokens': 46,       # Całkowita liczba tokenów (zapytanie + odpowiedź).\n",
    "        'completion_tokens_details': { \n",
    "            'accepted_prediction_tokens': 0,  # Tokeny zaakceptowane w predykcji (tu 0).\n",
    "            'audio_tokens': 0,                # Tokeny audio w odpowiedzi (tu 0, bo to tekst).\n",
    "            'reasoning_tokens': 0,            # Tokeny użyte do wnioskowania (tu 0).\n",
    "            'rejected_prediction_tokens': 0   # Tokeny odrzucone w predykcji (tu 0).\n",
    "        }, \n",
    "        'prompt_tokens_details': { \n",
    "            'audio_tokens': 0,                # Tokeny audio w zapytaniu (tu 0).\n",
    "            'cached_tokens': 0                # Tokeny z pamięci podręcznej (tu 0).\n",
    "        }\n",
    "    }, \n",
    "    'model_name': 'gpt-3.5-turbo-0125',  # Nazwa modelu użytego do generowania odpowiedzi.\n",
    "    'system_fingerprint': None,          # Odcisk palca systemu (tu None, brak danych).\n",
    "    'finish_reason': 'stop',             # Przyczyna zakończenia generowania (tu 'stop' - naturalne zakończenie).\n",
    "    'logprobs': None                     # Logarytmiczne prawdopodobieństwa tokenów (tu None, bo nie zwrócono).\n",
    "} \n",
    "\n",
    "# Unikalny identyfikator tej odpowiedzi.\n",
    "id='run-5e765e17-54de-4f6a-833e-bd2d2c284344-0' \n",
    "\n",
    "# Metadane zużycia - szczegółowe informacje o tokenach wejściowych i wyjściowych.\n",
    "usage_metadata={\n",
    "    'input_tokens': 34,                  # Liczba tokenów w zapytaniu.\n",
    "    'output_tokens': 12,                 # Liczba tokenów w odpowiedzi.\n",
    "    'total_tokens': 46,                  # Suma tokenów wejściowych i wyjściowych.\n",
    "    'input_token_details': { \n",
    "        'audio': 0,                      # Tokeny audio w zapytaniu (tu 0).\n",
    "        'cache_read': 0                  # Tokeny odczytane z pamięci podręcznej (tu 0).\n",
    "    }, \n",
    "    'output_token_details': { \n",
    "        'audio': 0,                      # Tokeny audio w odpowiedzi (tu 0).\n",
    "        'reasoning': 0                   # Tokeny użyte do wnioskowania (tu 0).\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b719ae6-8dcf-4e1b-b23a-85328f7cf268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate  # Import szablonu promptu z biblioteki LangChain\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate  # Import szablonu wiadomości systemowej dla modelu\n",
    "from langchain_openai.chat_models import ChatOpenAI  # Import klasy do interakcji z modelem OpenAI\n",
    "\n",
    "# Tworzenie szablonu promptu dla tłumaczenia językowego\n",
    "prompt = PromptTemplate(\n",
    "    template='''Jesteś pomocnym asystentem, który tłumaczy język {input_language} na {output_language}.''',\n",
    "    input_variables=[\"input_language\", \"output_language\"],  # Zmienna wejściowa dla języka źródłowego i docelowego\n",
    ")\n",
    "\n",
    "# Konwersja promptu na wiadomość systemową\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\n",
    "\n",
    "# Inicjalizacja modelu OpenAI do czatu\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# Wywołanie modelu z sformatowanym promptem, tłumaczącym z polskiego na angielski\n",
    "print(chat.invoke(system_message_prompt.format_messages(\n",
    "    input_language=\"polski\", output_language=\"angielski\"\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb3a50c-91fa-4921-b929-d3ec0ddeac4a",
   "metadata": {},
   "source": [
    "# Parsery wyjścia\n",
    "\n",
    "Pydantic to narzędzie do definiowania i walidacji struktur danych w Pythonie. Działa poprzez tworzenie klas (tzw. modeli), w których programista określa typy danych i ich ograniczenia. Biblioteka automatycznie sprawdza, czy dane wejściowe spełniają te wymagania, co czyni ją prostą, szybką i wygodną w użyciu. Główne zastosowania Pydantic to:\n",
    "\n",
    "Walidacja danych: Sprawdzanie, czy dane wejściowe (np. z formularzy czy API) są poprawne.\n",
    "Serializacja i deserializacja: Konwersja danych między obiektami Pythona a formatami takimi jak JSON.\n",
    "Zarządzanie konfiguracją: Definiowanie i walidowanie ustawień aplikacji.\n",
    "Pydantic jest szczególnie ceniony za łatwość integracji z nowoczesnymi narzędziami, takimi jak framework FastAPI, co czyni go popularnym wyborem w aplikacjach Pythona.\n",
    "\n",
    "Ten kod generuje nazwy firm na podstawie podanej branży i zestawu zasad.\n",
    "Wykorzystuje model OpenAI do stworzenia propozycji nazw firm, które następnie są walidowane i parsowane przy użyciu Pydantic.\n",
    "Można go użyć w aplikacjach do generowania nazw startupów, narzędziach wspomagających branding, a także w asystentach AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21d54949-d1d8-4c81-a7c3-a1075334d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names=[BusinessName(name='DataMind', rating_score=8.0), BusinessName(name='DataByte', rating_score=7.0), BusinessName(name='MindData', rating_score=9.0), BusinessName(name='ByteWise', rating_score=6.0), BusinessName(name='DataSync', rating_score=7.0)]\n",
      "names=[BusinessName(name='DataMinds', rating_score=9.2), BusinessName(name='ByteLab', rating_score=8.7), BusinessName(name='InfoNerd', rating_score=9.0), BusinessName(name='LearnData', rating_score=8.5), BusinessName(name='DataWiz', rating_score=8.8)]\n"
     ]
    }
   ],
   "source": [
    "# Importowanie potrzebnych klas z LangChain do tworzenia promptów dla czatu\n",
    "from langchain_core.prompts.chat import ( \n",
    "    ChatPromptTemplate,  # Klasa do tworzenia szablonów rozmów w czacie\n",
    "    SystemMessagePromptTemplate,  # Klasa do definiowania wiadomości systemowych, które określają zachowanie modelu\n",
    ")\n",
    "\n",
    "# Importowanie klasy ChatOpenAI z LangChain do integracji z modelem OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI  # Umożliwia komunikację z modelem językowym OpenAI\n",
    "\n",
    "# Importowanie parsera wyjścia, który przekształca tekstowe odpowiedzi modelu w struktury danych\n",
    "from langchain.output_parsers import PydanticOutputParser  # Parser zgodny z Pydantic, pozwala na walidację i strukturyzację danych\n",
    "\n",
    "# Importowanie klas z Pydantic do definiowania modeli danych z walidacją\n",
    "from pydantic import BaseModel, Field  # BaseModel to podstawa dla modeli danych, Field pozwala dodawać opisy i walidację pól\n",
    "\n",
    "# Importowanie typu List z typing do definiowania list w modelach Pydantic\n",
    "from typing import List  # Umożliwia określenie, że pole w modelu Pydantic będzie listą (np. listą nazw firm)\n",
    "\n",
    "# Ustawienie temperatury modelu na 0.0, co sprawia, że odpowiedzi są bardziej przewidywalne i mniej losowe\n",
    "temperature = 0.0  # Temperatura kontroluje kreatywność modelu; 0.0 = deterministyczne odpowiedzi\n",
    "\n",
    "# Definicja modelu Pydantic dla pojedynczej nazwy firmy\n",
    "class BusinessName(BaseModel):\n",
    "    name: str = Field(description=\"Nazwa firmy\")  # Pole 'name' typu string, przechowuje nazwę firmy\n",
    "    # Pole 'rating_score' typu float, ocena nazwy\n",
    "    rating_score: float = Field(description=\"Ocena firmy. 0 jest najgorsza, 10 jest najlepsza.\") \n",
    "    \n",
    "# Definicja modelu Pydantic dla listy nazw firm\n",
    "class BusinessNames(BaseModel):\n",
    "    names: List[BusinessName] = Field(description=\"Lista nazw firm\")  # Pole 'names' to lista obiektów BusinessName\n",
    "\n",
    "'''\n",
    "names=[BusinessName(name='DataWise', rating_score=8.0), \n",
    "       BusinessName(name='ByteLab', rating_score=7.5), \n",
    "       BusinessName(name='LearnData', rating_score=9.0), \n",
    "       BusinessName(name='DataMinds', rating_score=8.5), \n",
    "       BusinessName(name='InfoStat', rating_score=7.0)]\n",
    "       '''\n",
    "\n",
    "# Tworzenie parsera, który przekształci tekstowe wyjście modelu w obiekt BusinessNames\n",
    "parser = PydanticOutputParser(pydantic_object=BusinessNames)  # Parser wie, że ma parsować dane do struktury BusinessNames\n",
    "\n",
    "# Definicja zasad, które model musi przestrzegać przy generowaniu nazw\n",
    "principles = \"\"\"\n",
    "- Nazwa musi być łatwa do zapamiętania.  # Zasada: nazwy powinny być proste i chwytliwe\n",
    "- Skorzystaj z branży {industry} i kontekstu firmy, aby stworzyć dobrą nazwę  # Nazwy muszą pasować do branży (np. Nauka o danych)\n",
    "- Nazwa musi być łatwa do wymówienia.  # Unikamy skomplikowanych lub trudnych fonetycznie nazw\n",
    "- Musisz jedynie zwrócić nazwę bez żadnego dodatkowego tekstu lub znaków  # Model ma generować tylko nazwy, bez zbędnych znaków\n",
    "- Unikaj zwracania kropek, znaków nowej linii i innych tego typu znaków  # Odpowiedź ma być \"czysta\", bez formatowania\n",
    "- Maksymalna długość nazwy to 10 znaków  # Ograniczenie długości nazw do 10 znaków\n",
    "\"\"\"\n",
    "\n",
    "# Inicjalizacja modelu OpenAI, który będzie generował odpowiedzi\n",
    "model = ChatOpenAI()  # Tworzy instancję modelu OpenAI (domyślnie używa API OpenAI)\n",
    "\n",
    "# Szablon promptu, który definiuje zadanie dla modelu\n",
    "template = \"\"\"Wygeneruj pięć nazw firm dla nowego startupu w branży {industry}\n",
    "Musisz podążać według następujących zasad: {principles}\n",
    "{format_instructions}\n",
    "\"\"\"  # Szablon zawiera zmienne do wstrzyknięcia: branża, zasady i instrukcje formatowania\n",
    "\n",
    "# Tworzenie promptu systemowego na podstawie szablonu\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)  # Przekształca szablon w prompt systemowy\n",
    "\n",
    "# Tworzenie pełnego promptu czatu, który zawiera tylko wiadomość systemową\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])  # Tworzy strukturę promptu gotową do użycia z modelem\n",
    "\n",
    "# Tworzenie łańcucha: prompt jest przesyłany do modelu\n",
    "prompt_and_model = chat_prompt | model  # Operator | łączy prompt z modelem w tzw. łańcuch LCEL (LangChain Expression Language)\n",
    "\n",
    "# Wywołanie modelu z konkretnymi wartościami dla zmiennych w promptcie\n",
    "result = prompt_and_model.invoke(\n",
    "    {\n",
    "        \"principles\": principles,  # Wstrzyknięcie zasad do promptu\n",
    "        \"industry\": \"Nauka o danych\",  # Określenie branży jako \"Nauka o danych\"\n",
    "        \"format_instructions\": parser.get_format_instructions(),  # Dodanie instrukcji formatowania wygenerowanych przez parser\n",
    "    }\n",
    ")  # Model generuje odpowiedź na podstawie podanych danych\n",
    "\n",
    "# Parsowanie tekstowej odpowiedzi modelu na obiekt Pydantic i wyświetlenie wyniku\n",
    "print(parser.parse(result.content))  # result.content to tekst wygenerowany przez model, parser przekształca go w BusinessNames\n",
    "\n",
    "# Ponowne utworzenie parsera (nadpisywanie poprzedniego, można pominąć jeśli używamy tego samego)\n",
    "parser = PydanticOutputParser(pydantic_object=BusinessNames)  # Tworzy nowy parser dla BusinessNames (tu redundantne, ale zachowane jak w oryginalnym kodzie)\n",
    "\n",
    "# Tworzenie pełnego łańcucha: prompt -> model -> parser\n",
    "chain = chat_prompt | model | parser  # Łańcuch automatycznie parsuje wyjście modelu na obiekt Pydantic\n",
    "\n",
    "# Wywołanie pełnego łańcucha z tymi samymi parametrami\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"principles\": principles,  # Wstrzyknięcie zasad\n",
    "        \"industry\": \"Nauka o danych\",  # Branża\n",
    "        \"format_instructions\": parser.get_format_instructions(),  # Instrukcje formatowania\n",
    "    }\n",
    ")  # Łańcuch zwraca już sparsowany obiekt BusinessNames\n",
    "\n",
    "# Wyświetlenie sparsowanego wyniku\n",
    "print(result)  # Wyświetla obiekt BusinessNames z nazwami i ocenami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84fac392-7b0f-4667-be89-b3c64227e64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DataMinds', 'ByteLab', 'InfoNerd', 'LearnData', 'DataWiz']\n"
     ]
    }
   ],
   "source": [
    "names_list = [i.name for i in result.names]\n",
    "print(names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c115020-d791-4530-96f5-2153e3df5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names=[BusinessName(name='DataWise', rating_score=8.0), \n",
    "#        BusinessName(name='ByteLab', rating_score=7.5), \n",
    "#        BusinessName(name='LearnData', rating_score=9.0), \n",
    "#        BusinessName(name='DataMinds', rating_score=8.5), \n",
    "#        BusinessName(name='InfoStat', rating_score=7.0)]\n",
    "\n",
    "# names=[BusinessName(name='DataMinds', rating_score=8.0), \n",
    "#        BusinessName(name='ByteIQ', rating_score=7.0), \n",
    "#        BusinessName(name='InfoGenix', rating_score=9.0), \n",
    "#        BusinessName(name='DataNest', rating_score=8.0), \n",
    "#        BusinessName(name='SmartData', rating_score=7.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d311d4-dcae-493f-8894-ad1392d500a4",
   "metadata": {},
   "source": [
    "# Ewaluacje LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03f768a2-632c-45b8-a530-5db926d97297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cash deposit at local branch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cash deposit at local branch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>withdrew money for rent payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>withdrew cash for weekend expenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>purchased books from the bookstore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Transaction Description\n",
       "0        cash deposit at local branch\n",
       "1        cash deposit at local branch\n",
       "2     withdrew money for rent payment\n",
       "3  withdrew cash for weekend expenses\n",
       "4  purchased books from the bookstore"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Dataset URL:\n",
    "url = \"https://storage.googleapis.com/oreilly-content/transaction_data_with_expanded_descriptions.csv\"\n",
    "\n",
    "# Download the file from the URL:\n",
    "downloaded_file = requests.get(url)\n",
    "\n",
    "# Load the transactions dataset and only look at 20 transactions:\n",
    "df = pd.read_csv(io.StringIO(downloaded_file.text))[:20]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24bb580c-6977-4864-8fd5-2824912809a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:23<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser    \n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Union\n",
    "\n",
    "# 1. Define the model:\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"You are are an expert at analyzing bank transactions, \n",
    "you will be categorising a single transaction. \n",
    "Always return a transaction type and category: do not return None.\n",
    "Format Instructions:\n",
    "{format_instructions}\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Transaction Text:\n",
    "{transaction}\"\"\"\n",
    "\n",
    "# 2. Define the prompt:\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            user_prompt,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. Define the pydantic model:\n",
    "class EnrichedTransactionInformation(BaseModel):\n",
    "    transaction_type: Union[\n",
    "        Literal[\"Purchase\", \"Withdrawal\", \"Deposit\", \"Bill Payment\", \"Refund\"], None\n",
    "    ]\n",
    "    transaction_category: Union[\n",
    "        Literal[\"Food\", \"Entertainment\", \"Transport\", \"Utilities\", \"Rent\", \"Other\"],\n",
    "        None,\n",
    "    ]\n",
    "\n",
    "\n",
    "# 4. Define the output parser:\n",
    "output_parser = PydanticOutputParser(pydantic_object=EnrichedTransactionInformation)\n",
    "\n",
    "# 5. Create an LCEL chain:\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 6. Invoke the chain for the whole dataset:\n",
    "results = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    transaction = row[\"Transaction Description\"]\n",
    "    result = chain.invoke(\n",
    "        {\n",
    "            \"transaction\": transaction,\n",
    "            \"format_instructions\": output_parser.get_format_instructions(),\n",
    "        }\n",
    "    )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fe96ae6-b517-46b3-a2a4-2fe3ccd6e399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction Description</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>transaction_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cash deposit at local branch</td>\n",
       "      <td>Deposit</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cash deposit at local branch</td>\n",
       "      <td>Deposit</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>withdrew money for rent payment</td>\n",
       "      <td>Withdrawal</td>\n",
       "      <td>Rent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>withdrew cash for weekend expenses</td>\n",
       "      <td>Withdrawal</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>purchased books from the bookstore</td>\n",
       "      <td>Purchase</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Transaction Description transaction_type transaction_category\n",
       "0        cash deposit at local branch          Deposit                Other\n",
       "1        cash deposit at local branch          Deposit                Other\n",
       "2     withdrew money for rent payment       Withdrawal                 Rent\n",
       "3  withdrew cash for weekend expenses       Withdrawal                Other\n",
       "4  purchased books from the bookstore         Purchase                Other"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_types = []\n",
    "transaction_categories = []\n",
    "\n",
    "for result in results:\n",
    "    transaction_types.append(result.transaction_type)\n",
    "    transaction_categories.append(result.transaction_category)\n",
    "\n",
    "df[\"transaction_type\"] = transaction_types\n",
    "df[\"transaction_category\"] = transaction_categories\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80fed615-a46a-4bbf-a4d5-801398d1df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"transactions_with_enriched_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
